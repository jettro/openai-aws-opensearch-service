{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain\n",
    "#%pip install unstructured\n",
    "#%pip install markdown\n",
    "#%pip install pandas\n",
    "#%pip install opensearch-py\n",
    "#%pip install boto3\n",
    "#%pip install requests-aws4auth\n",
    "#%pip install python-dotenv\n",
    "#%pip install openai\n",
    "#%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import content\n",
    "\n",
    "Start by importing content. The content can be used to create embeddings and store in a vector store.\n",
    "\n",
    "At the moment there are three files that are in the format of smaller blocks of text in a file. The following files with content are available:\n",
    "- help-account.txt\n",
    "- help-search.txt\n",
    "- help-sustainability.txt\n",
    "\n",
    "In the end, all texts are store in an array called _available_texts_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:37:39.131425Z",
     "start_time": "2023-08-24T10:37:39.113198Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_up_file(file_name: str):\n",
    "    with open(file_name) as split_file:\n",
    "        help_account = split_file.read()\n",
    "\n",
    "    # Problem with this splitter is that it splits, but also merges if the chunk size is not reached. \n",
    "    # BY choosing a small chunk it work fine. But do test id this is what you need.\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\",\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    return text_splitter.create_documents([help_account])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:37:58.646943Z",
     "start_time": "2023-08-24T10:37:58.623472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 310, which is longer than the specified 200\n",
      "Created a chunk of size 278, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 205, which is longer than the specified 200\n",
      "Created a chunk of size 218, which is longer than the specified 200\n",
      "Created a chunk of size 253, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# available_texts = (split_up_file('./help-account.txt')\n",
    "#                    + split_up_file('./help-search.txt')\n",
    "#                    + split_up_file('./help-sustainability.txt'))\n",
    "available_texts = (split_up_file('./help-account.txt'))\n",
    "\n",
    "print(len(available_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is one other file that has a different structure. This is the shop-locations file. As the structure is different, we made a special importer. The function generates an array of dictionaries containing the available information for a shop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T22:21:43.518220Z",
     "start_time": "2023-08-21T22:21:43.506695Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.staging.base import convert_to_dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T22:22:20.787604Z",
     "start_time": "2023-08-21T22:22:20.781989Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_store_line(input_string):\n",
    "    # Split the input string by newline and colon\n",
    "    lines = input_string.split('\\n')\n",
    "    data_dict = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            # Replace spaces with underscores in keys and convert to lowercase\n",
    "            key = key.lower().replace(' ', '_')\n",
    "\n",
    "            data_dict[key] = value\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def extract_stores():\n",
    "    filename = os.path.join('./', 'help-shop-locations.txt')\n",
    "    with open(filename, \"rb\") as f:\n",
    "        elements = partition_md(filename=filename)\n",
    "\n",
    "    elements_dict = convert_to_dict(elements)\n",
    "\n",
    "    available_stores = []\n",
    "    current_store = {}\n",
    "    for el in elements_dict:\n",
    "        line_dict = extract_store_line(el[\"text\"])\n",
    "        if \"store_name\" in line_dict:\n",
    "            available_stores.append(current_store)\n",
    "            line_dict[\"opening_hours\"] = []\n",
    "            current_store = line_dict\n",
    "        else:\n",
    "            if current_store.get(\"opening_hours\"):\n",
    "                current_store[\"opening_hours\"].append(line_dict)\n",
    "            else:\n",
    "                current_store[\"opening_hours\"] = [line_dict]\n",
    "    return available_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T22:22:22.970972Z",
     "start_time": "2023-08-21T22:22:22.949585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}, {'store_name': 'Trendy Finds', 'city': 'Pijnacker', 'street': 'Kerkweg 1', 'telephone': '06-12345678', 'opening_hours': [{'monday': '11:00-18:00'}, {'tuesday': '09:00-18:00'}, {'wednesday': '09:00-18:00'}, {'thursday': '11:00-18:00'}, {'friday': '09:00-18:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}, {'store_name': 'Urban Chic Emporium', 'city': 'Rotterdam', 'street': 'Hoogstraat 23', 'telephone': '06-12345678', 'opening_hours': [{'monday': '10:00-18:00'}, {'tuesday': '10:00-18:00'}, {'wednesday': '10:00-18:00'}, {'thursday': '10:00-20:00'}, {'friday': '10:00-18:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}, {'store_name': 'The Boutique Haven', 'city': 'Ghent', 'street': 'Veldstraat 8', 'telephone': '06-12345678', 'opening_hours': [{'monday': '09:30-18:00'}, {'tuesday': '09:30-18:00'}, {'wednesday': '09:30-18:00'}, {'thursday': '09:30-18:00'}, {'friday': '09:30-20:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}, {'store_name': 'Chic & Cozy', 'city': 'Antwerp', 'street': 'Meir 12', 'telephone': '06-12345678', 'opening_hours': [{'monday': '10:30-18:00'}, {'tuesday': '10:30-18:00'}, {'wednesday': '10:30-18:00'}, {'thursday': '10:30-18:00'}, {'friday': '10:30-20:00'}, {'saturday': '10:00-17:30'}, {'sunday': 'Closed'}]}, {'store_name': 'Style Haven', 'city': 'Utrecht', 'street': 'Oudegracht 45', 'telephone': '06-12345678', 'opening_hours': [{'monday': '11:00-18:00'}, {'tuesday': '11:00-18:00'}, {'wednesday': '11:00-18:00'}, {'thursday': '11:00-18:00'}, {'friday': '11:00-20:00'}, {'saturday': '10:30-17:30'}, {'sunday': 'Closed'}]}, {'store_name': 'Elegance Essentials', 'city': 'Amsterdam', 'street': 'Kalverstraat 76', 'telephone': '06-12345678', 'opening_hours': [{'monday': '10:00-18:00'}, {'tuesday': '10:00-18:00'}, {'wednesday': '10:00-18:00'}, {'thursday': '10:00-18:00'}, {'friday': '10:00-20:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}, {'store_name': 'Modern Trends', 'city': 'Brussels', 'street': 'Avenue Louise 112', 'telephone': '06-12345678', 'opening_hours': [{'monday': '09:00-18:00'}, {'tuesday': '09:00-18:00'}, {'wednesday': '09:00-18:00'}, {'thursday': '09:00-18:00'}, {'friday': '09:00-20:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}, {'store_name': 'Fashion Fusion', 'city': 'Eindhoven', 'street': 'Demer 21', 'telephone': '06-12345678', 'opening_hours': [{'monday': '11:00-18:00'}, {'tuesday': '11:00-18:00'}, {'wednesday': '11:00-18:00'}, {'thursday': '11:00-18:00'}, {'friday': '11:00-20:00'}, {'saturday': '10:30-17:30'}, {'sunday': 'Closed'}]}, {'store_name': 'Urban Elegance', 'city': 'Leuven', 'street': 'Bondgenotenlaan 7', 'telephone': '06-12345678', 'opening_hours': [{'monday': '10:00-18:00'}, {'tuesday': '10:00-18:00'}, {'wednesday': '10:00-18:00'}, {'thursday': '10:00-18:00'}, {'friday': '10:00-20:00'}, {'saturday': '10:00-17:00'}, {'sunday': 'Closed'}]}]\n"
     ]
    }
   ],
   "source": [
    "found_stores = extract_stores()\n",
    "print(found_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the products that are available in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T12:44:08.587226Z",
     "start_time": "2023-08-24T12:44:07.958130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Position                                       product_link  age  \\\n0         1  https://www.lego.com/nl-nl/product/professors-...  10+   \n1         2  https://www.lego.com/nl-nl/product/harry-hermi...  10+   \n2         3  https://www.lego.com/nl-nl/product/chip-dale-4...  10+   \n3         4  https://www.lego.com/nl-nl/product/woody-and-b...  10+   \n4         5  https://www.lego.com/nl-nl/product/goofy-pluto...  10+   \n\n   number_of_pieces                            title   price  \\\n0               601          Leraren van Zweinstein™  €39,99   \n1               466  Harry, Hermelien, Ron & Hagrid™  €24,99   \n2               226                 Knabbel & Babbel  €19,99   \n3               296                  Woody & Bo Peep  €19,99   \n4               214                   Goofy en Pluto  €14,99   \n\n                                          image_link  \\\n0  https://www.lego.com/cdn/cs/set/assets/blt8c72...   \n1  https://www.lego.com/cdn/cs/set/assets/bltbc78...   \n2  https://www.lego.com/cdn/cs/set/assets/blt0bac...   \n3  https://www.lego.com/cdn/cs/set/assets/blt2bea...   \n4  https://www.lego.com/cdn/cs/set/assets/blt4306...   \n\n                                 product_description image_name  id  \n0  Dit is een betoverende verrassing voor fans va...  40560.png   1  \n1  LEGO® BrickHeadz™ versies van 4 van de bekends...  40495.jpg   2  \n2  Keer terug naar je jeugd met deze leuke LEGO® ...  40550.png   3  \n3  Zorg dat je twee favoriete filmpersonages alti...  40553.png   4  \n4  Deze Goofy en Pluto set (40378) met 2 klassiek...  40378.jpg   5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Position</th>\n      <th>product_link</th>\n      <th>age</th>\n      <th>number_of_pieces</th>\n      <th>title</th>\n      <th>price</th>\n      <th>image_link</th>\n      <th>product_description</th>\n      <th>image_name</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>https://www.lego.com/nl-nl/product/professors-...</td>\n      <td>10+</td>\n      <td>601</td>\n      <td>Leraren van Zweinstein™</td>\n      <td>€39,99</td>\n      <td>https://www.lego.com/cdn/cs/set/assets/blt8c72...</td>\n      <td>Dit is een betoverende verrassing voor fans va...</td>\n      <td>40560.png</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>https://www.lego.com/nl-nl/product/harry-hermi...</td>\n      <td>10+</td>\n      <td>466</td>\n      <td>Harry, Hermelien, Ron &amp; Hagrid™</td>\n      <td>€24,99</td>\n      <td>https://www.lego.com/cdn/cs/set/assets/bltbc78...</td>\n      <td>LEGO® BrickHeadz™ versies van 4 van de bekends...</td>\n      <td>40495.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>https://www.lego.com/nl-nl/product/chip-dale-4...</td>\n      <td>10+</td>\n      <td>226</td>\n      <td>Knabbel &amp; Babbel</td>\n      <td>€19,99</td>\n      <td>https://www.lego.com/cdn/cs/set/assets/blt0bac...</td>\n      <td>Keer terug naar je jeugd met deze leuke LEGO® ...</td>\n      <td>40550.png</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>https://www.lego.com/nl-nl/product/woody-and-b...</td>\n      <td>10+</td>\n      <td>296</td>\n      <td>Woody &amp; Bo Peep</td>\n      <td>€19,99</td>\n      <td>https://www.lego.com/cdn/cs/set/assets/blt2bea...</td>\n      <td>Zorg dat je twee favoriete filmpersonages alti...</td>\n      <td>40553.png</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>https://www.lego.com/nl-nl/product/goofy-pluto...</td>\n      <td>10+</td>\n      <td>214</td>\n      <td>Goofy en Pluto</td>\n      <td>€14,99</td>\n      <td>https://www.lego.com/cdn/cs/set/assets/blt4306...</td>\n      <td>Deze Goofy en Pluto set (40378) met 2 klassiek...</td>\n      <td>40378.jpg</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('./small.csv')\n",
    "df = pd.read_csv('./extract-data-brickheadz.csv')\n",
    "df[\"image_name\"] = df[\"image_link\"].str.rsplit('/').str.get(-1)\n",
    "df[\"id\"] = df[\"Position\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we download the images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for key, value in df.iterrows():\n",
    "    url = value.image_link\n",
    "    image_name = url.rsplit('/', 1)[-1]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(\"images/\" + image_name, \"wb\") as f:\n",
    "        f.write(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-23T10:08:32.254551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a connection to the Amazon OpenSearch Cluster\n"
     ]
    }
   ],
   "source": [
    "from retriever import find_auth_opensearch, OpenSearchClient\n",
    "\n",
    "config = find_auth_opensearch()\n",
    "client = OpenSearchClient(config, alias_name=\"sg-products\")\n",
    "\n",
    "if client.ping():\n",
    "    print(\"We have a connection to the Amazon OpenSearch Cluster\")\n",
    "else:\n",
    "    print(\"ERROR: no connection to the Amazon OpenSearch Cluster\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:44:20.359610Z",
     "start_time": "2023-08-24T12:44:16.622929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version 2 of the component template sg_product_component_settings is up-to-date\n",
      "The version 1 of the component template sg_product_component_dynamic_mappings is up-to-date\n",
      "Update the component template sg_product_component_mappings to version 1.\n",
      "Update the template to version 3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from retriever import OpenSearchTemplate\n",
    "\n",
    "template = OpenSearchTemplate(\n",
    "    client=client,\n",
    "    index_template_name=\"sg_product_index_template\",\n",
    "    component_name_settings=\"sg_product_component_settings\",\n",
    "    component_name_dyn_mappings=\"sg_product_component_dynamic_mappings\",\n",
    "    component_name_mappings=\"sg_product_component_mappings\"\n",
    ")\n",
    "\n",
    "for result in template.create_update_template():\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:45:54.427487Z",
     "start_time": "2023-08-24T12:45:54.068525Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading content\n",
    "Loading the content is a tricky beast. They make it feel so easy. You have a document, do some chunking, create embeddings, store the embeddings in a vector store and do similarity search. Having a extensive search background, there are so many facets to return relevant results, also for semantic search. Often you want more structure in your content.\n",
    "\n",
    "To have more control, we are indexing the documents ourselves. We do use some Langchain components to make our life easier.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "index_name = client.create_index()\n",
    "print(f\"Index created with the name {index_name}\")\n",
    "# for key, value in df.iterrows():\n",
    "#     client.index_product(index_name=index_name, product=value.to_dict())\n",
    "\n",
    "client.switch_alias_to(index_name=index_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:46:23.601687Z",
     "start_time": "2023-08-24T12:46:23.371296Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we use Langchain to index some of the help content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# print(os.getenv('OPEN_AI_API_KEY'))\n",
    "\n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv('OPEN_AI_API_KEY')),\n",
    "    opensearch_url=f\"https://{config['host']}:{config['port']}\",\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    http_auth=config[\"auth\"],\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:47:00.833293Z",
     "start_time": "2023-08-24T12:46:57.890900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the vector store in place, we can start indexing documents. You can use the kwargs to configure some of the engine specific aspects:\n",
    "- text_field: Name of the field to store the text in\n",
    "- vector_field: Name of the field to store the vector in\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vector_store.add_texts(texts=df[\"title\"].to_list(), metadatas=df.to_dict('records'), ids=df[\"id\"].to_list(), text_field=\"title\", vector_field=\"title_vector\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:47:09.514920Z",
     "start_time": "2023-08-24T12:47:07.383986Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to query the vector store to see if it works better than lexical search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results from: OpenSearch\n",
      "0.7957365 - Donald Duck\n",
      "0.75702626 - Vaiana & Merida\n",
      "0.74488837 - Cruella & Maleficent\n",
      "0.7438165 - Goofy en Pluto\n"
     ]
    }
   ],
   "source": [
    "found_docs = vector_store.similarity_search_with_score(query=\"disney\",text_field=\"title\", vector_field=\"title_vector\")\n",
    "print(f\"\\nResults from: OpenSearch\")\n",
    "for doc, _score in found_docs:\n",
    "    print(f\"{_score} - {doc.page_content}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T13:00:52.121167Z",
     "start_time": "2023-08-24T13:00:51.676115Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
