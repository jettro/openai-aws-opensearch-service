{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import content\n",
    "\n",
    "Start by importing content. The content can be used to create embeddings and store in a vector store.\n",
    "\n",
    "At the moment there are three files that are in the format of smaller blocks of text in a file. The following files with content are available:\n",
    "- help-account.txt\n",
    "- help-search.txt\n",
    "- help-sustainability.txt\n",
    "\n",
    "After importing the texts, we create chunks out of them. We use a Langchain text splitter. The splitter uses specific combination of characters to break up strings. It uses the chunk size to combine strings but stay within the chunk size.  \n",
    "\n",
    "In the end, all chunks are stored in an array called _available_texts_. This array is used as input for the vector store based on OpenSearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:28:38.176007Z",
     "start_time": "2023-09-03T21:28:38.171834Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_up_file(file_name: str):\n",
    "    \"\"\"Returns a list of Langchain documents, each containing a chunk if the text in the provided file.\"\"\"\n",
    "    with open(file_name) as split_file:\n",
    "        help_account = split_file.read()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    return text_splitter.create_documents([help_account])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:28:40.082117Z",
     "start_time": "2023-09-03T21:28:40.075331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of found chunks in the three files is 34\n"
     ]
    }
   ],
   "source": [
    "available_texts = (split_up_file('./help-account.txt')\n",
    "                   + split_up_file('./help-search.txt')\n",
    "                   + split_up_file('./help-sustainability.txt'))\n",
    "\n",
    "print(f\"Number of found chunks in the three files is {len(available_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialise the client to manage the index templates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a connection to the Amazon OpenSearch Cluster\n"
     ]
    }
   ],
   "source": [
    "from retriever import find_auth_opensearch, OpenSearchClient\n",
    "\n",
    "config = find_auth_opensearch()\n",
    "client = OpenSearchClient(config, alias_name=\"sg-content\")\n",
    "\n",
    "if client.ping():\n",
    "    print(\"We have a connection to the Amazon OpenSearch Cluster\")\n",
    "else:\n",
    "    print(\"ERROR: no connection to the Amazon OpenSearch Cluster\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:41:08.765083Z",
     "start_time": "2023-09-03T21:41:06.999Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update the component template sg_content_component_settings to version 2.\n",
      "Update the component template sg_content_component_dynamic_mappings to version 1.\n",
      "Update the component template sg_content_component_mappings to version 1.\n",
      "Update the template to version 3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from retriever import OpenSearchTemplate\n",
    "\n",
    "template = OpenSearchTemplate(\n",
    "    client=client,\n",
    "    index_template_name=\"sg_content_index_template\",\n",
    "    component_name_settings=\"sg_content_component_settings\",\n",
    "    component_name_dyn_mappings=\"sg_content_component_dynamic_mappings\",\n",
    "    component_name_mappings=\"sg_content_component_mappings\"\n",
    ")\n",
    "\n",
    "for result in template.create_update_template():\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-02T23:24:49.190934Z",
     "start_time": "2023-09-02T23:24:48.825678Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading content\n",
    "Loading the content is a tricky beast. They make it feel so easy. You have a document, do some chunking, create embeddings, store the embeddings in a vector store and do similarity search. Having a extensive search background, there are so many facets to return relevant results, also for semantic search. Often you want more structure in your content.\n",
    "\n",
    "To have more control, we are indexing the documents ourselves. We do use some Langchain components to make our life easier.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created with the name sg-content-20230903012633\n"
     ]
    }
   ],
   "source": [
    "index_name = client.create_index()\n",
    "print(f\"Index created with the name {index_name}\")\n",
    "\n",
    "client.switch_alias_to(index_name=index_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-02T23:26:34.121205Z",
     "start_time": "2023-09-02T23:26:33.894660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we use Langchain to index some of the help content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    index_name=index_name,\n",
    "    embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv('OPEN_AI_API_KEY')),\n",
    "    opensearch_url=f\"https://{config['host']}:{config['port']}\",\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    http_auth=config[\"auth\"],\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:41:12.374059Z",
     "start_time": "2023-09-03T21:41:12.367419Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the vector store in place, we can start indexing documents. You can use the kwargs to configure some of the engine specific aspects:\n",
    "- text_field: Name of the field to store the text in\n",
    "- vector_field: Name of the field to store the vector in\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "['69314e26-f5e3-498f-a337-a8121cdc9a51',\n '884b4061-3ddd-4b67-b026-a852c0d858fe',\n '320f109c-4c57-49a2-86f0-4cda952a988e',\n '8f16b3e7-a29e-4b89-b398-9092e6cb3c45',\n 'a841ad7f-d046-4e40-851e-c52e5b667161',\n 'b5e925bc-fc1f-46cc-bfe7-d7939a0266bd',\n 'e4c2d1ec-e07b-4a01-bcc9-fa94437e083a',\n '206a4079-1676-4b65-937e-f6171e15dd6d',\n 'ebbea6dd-c62d-4527-92fe-63ec77d2e6d1',\n '92f6cc1c-b16d-4484-a236-db08ce434df1',\n '5c593658-1417-4042-bea2-43e9cb08f944',\n '0f52dc6e-a296-47a8-ae0a-3794fca3e8dd',\n '3078bd3e-8a36-4202-81ec-c8aa9ffdd203',\n 'e3a15d0e-cb60-4e66-a735-e087d1c45b15',\n '765fb3d0-6d89-4198-8d69-2354f466ae1b',\n '43dc8dff-8dc3-451a-8bd6-6238b784af73',\n '81106da5-25b2-445a-8b1d-c51a66217fe4',\n 'd00a1ebc-14f5-4cd0-9c6a-386bd44031b5',\n '0fbebeac-1435-4ac8-860c-24b6edd404e4',\n 'e767ecfe-916f-4135-882b-e386508f604c',\n '07e95d1d-9d50-4ae0-87fb-66f475225dd3',\n '99b589ea-425c-4160-8221-ad0db9725bb7',\n 'd34a5c28-2dda-41ed-8bb5-c9837a0942a3',\n '4bc10745-0fb5-47c1-96ab-a15065dd62a1',\n 'd299e5ad-5cad-48ac-9e42-4eb4dbd5fea7',\n '77b8a5af-f6d8-4ee2-8c50-81b968faf4b0',\n '8561a399-44a9-4233-a924-b46516689694',\n 'e85ee117-b7eb-4361-b787-deed86131ded',\n 'cfbd1b2c-fce6-4571-88d1-66ffff43a5b0',\n 'd533a98d-8db1-4984-8335-154bd5b56556',\n '3da3c323-372d-4cfd-abef-249c6522cff2',\n 'adc1346d-f489-4111-a090-69309a0e0d9c',\n '8f1a460d-bc5c-4f7c-bc22-bb550d98d2f8',\n '13eabae0-b6d1-4458-9f8d-686cf05a44ff']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vector_store.add_documents(documents=available_texts)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-02T23:26:43.898023Z",
     "start_time": "2023-09-02T23:26:42.520389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search for answers\n",
    "\n",
    "Now we want to query the vector store to see if it works better than lexical search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results from: OpenSearch\n",
      "---\n",
      "0.70916265 - Upon conducting a search, a list of products will be displayed. You can further narrow down these results using filters located on the side of the page. Filters include options like category, price range, brand, size, and color.\n",
      "---\n",
      "---\n",
      "0.69707394 - For organized viewing, sort the search results by criteria like relevance, price, or popularity using the sorting dropdown menu near the top of the page.\n",
      "---\n",
      "---\n",
      "0.68750495 - Remember that some platforms allow you to save your search criteria for future reference, streamlining your shopping experience.\n",
      "\n",
      "Utilize these steps to effortlessly navigate our ecommerce website and successfully discover the products you're searching for.\n",
      "---\n",
      "---\n",
      "0.6831702 - As you type in the search bar, you may notice search suggestions that can expedite the process.\n",
      "\n",
      "For more specific searches, consider using the advanced search option if available. This enables refining searches using specialized criteria.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "found_docs = vector_store.similarity_search_with_score(query=\"Do you support filters on your website?\")\n",
    "print(f\"\\nResults from: OpenSearch\")\n",
    "for doc, _score in found_docs:\n",
    "    print(\"---\")\n",
    "    print(f\"{_score} - {doc.page_content}\")\n",
    "    print(\"---\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:41:17.260182Z",
     "start_time": "2023-09-03T21:41:16.584268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template = \"\"\"Use the context to answer the question. If you don't know the \n",
    "    answer, just say that you don't know, don't make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": custom_prompt}\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(openai_api_key=os.getenv('OPEN_AI_API_KEY')),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:42:14.141611Z",
     "start_time": "2023-09-03T21:42:14.131375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"What are the opening times for the store in Amsterdam\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T21:42:51.307793Z",
     "start_time": "2023-09-03T21:42:50.231126Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
